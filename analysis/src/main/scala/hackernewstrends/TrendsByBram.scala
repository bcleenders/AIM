package hackernewstrends

import edu.arizona.sista.processors.fastnlp.FastNLPProcessor
import org.apache.spark.SparkContext
import org.apache.spark.mllib.clustering.LDA
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD
import org.json4s.jackson.JsonMethods._
import org.json4s.{DefaultFormats, _}

import scala.collection.mutable

object TrendsByBram extends App {
  // <Settings>
  val inputPath = "/Users/bramleenders/Desktop/output_awesome/"
  val fileFilter = "HN-stories-2015-January-*"
  val outputPath = "/Users/bramleenders/Desktop/analysis_output/"
  // </Settings>

  implicit val formats = DefaultFormats

  // Brings in default date formats etc.
  case class WebPage(title: String, metaDescription: String, metaKeywords: String, cleanedText: String, finalUrl: String, topImage: String)

  case class HNItem(created_at: java.util.Date, title: String, url: String, author: String, points: Int, story_text: String,
                    num_comments: Int, created_at_i: Int, objectID: String)

  case class Item(webpage: WebPage, HNItem: HNItem)

  // Load the stop words
  // List found on: http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop
  val stream = getClass.getResourceAsStream("/english.stop")
  val stopWords = scala.io.Source.fromInputStream(stream).getLines().toList

  val sc = new SparkContext("local[8]", "Main")

  // Load all the files generated by the crawler and split each line (each line contains one article)
  // Get rid of weird characters and filter all the empty artciles (the ones that the crawler couldn't fetch)
  val corpus: RDD[Item] = sc.wholeTextFiles(inputPath + fileFilter).flatMap { case (_, file) =>
    file.split("\n").map(parse(_).extract[Item]).filter(_.webpage.cleanedText != "")
  }

  // Init the NLPProcessor (other options here: CoreNLPProcessor, BioNLPProcessor)
  val proc = new FastNLPProcessor(withDiscourse = true)
  val tokenized = corpus.map(_.webpage.cleanedText)
  .map(_.toLowerCase)
  .map(_.replaceAll("(?m)[^a-z0-9\\.]+", " "))
  .mapPartitions(partition => {
    partition.map { p =>
      val doc = proc.mkDocument(p)
      proc.tagPartsOfSpeech(doc)
      proc.lemmatize(doc)
      val words = doc.sentences.flatMap(x => x.lemmas.get)
//      doc.clear()
      val size = words.length

//      println("Lemmatized " + words.length + " words.")

      words
    }
  })
    .map(
      _.filter(_.length > 3) // Only words that are longer than 3 characters
      .filter(!stopWords.contains(_)) // Filter out the stop-words
    )

  // Choose the vocabulary.
  //   termCounts: Sorted list of (term, termCount) pairs
  val termCounts: Array[(String, Long)] =
    tokenized
      .flatMap(_.map(_ -> 1L))
      .reduceByKey(_ + _)
      .collect()
      .sortBy(-_._2)

  //   vocabArray: Chosen vocab (removing common terms)
  val numStopwords = 20
  val vocabArray: Array[String] =
    termCounts
      .takeRight(termCounts.size - numStopwords)
      .map(_._1)

  //   vocab: Map term -> term index
  val vocab: Map[String, Int] = vocabArray.zipWithIndex.toMap

  // Convert documents into term count vectors
  val documents: RDD[(Long, Vector)] =
    tokenized.zipWithIndex().map { case (tokens, id) =>
      val counts = new mutable.HashMap[Int, Double]()
      tokens.foreach { term =>
        if (vocab.contains(term)) {
          val idx = vocab(term)
          counts(idx) = counts.getOrElse(idx, 0.0) + 1.0
        }
      }
      (id, Vectors.sparse(vocab.size, counts.toSeq))
    }

  // Set LDA parameters
  val numTopics = 50
  val lda = new LDA().setK(numTopics).setMaxIterations(50)

  val ldaModel = lda.run(documents)
  val avgLogLikelihood = ldaModel.logLikelihood / documents.count()

  // Write topics, showing top-weighted 10 terms for each topic.
  val outTM = new java.io.FileWriter(outputPath + "Topic_Matrix_" + fileFilter + ".txt")
  var counter = 0

  // Print topics, showing top-weighted 10 terms for each topic.
  val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)
  topicIndices.foreach { case (terms, termWeights) =>
    outTM.write(s"TOPIC $counter:\n")
    counter = counter + 1
    terms.zip(termWeights).foreach { case (term, weight) =>
      outTM.write(s"${vocabArray(term.toInt)}\t$weight\n")
    }
    outTM.write("\n")
  }
  outTM.close()

  // Write top topics for each document
  val outTD = new java.io.FileWriter(outputPath + "Topic_Distributions_" + fileFilter + ".txt")

  val topicDist = ldaModel.topicDistributions.collect()
  corpus.collect().map(_.HNItem.title).zip(topicDist.sortBy(_._1)).foreach { case (docID, (_, topics)) =>
    outTD.write(docID)
    topics.toArray.zipWithIndex.sortBy(-_._1).filter(_._1 >= 1.0 / numTopics).foreach { case (_, topicID) =>
      outTD.write(", " + topicID)
    }
    outTD.write("\n")
  }

  outTD.close()

  sc.stop()
}
