package hackernewstrends

import com.github.mauricio.async.db.pool.{ConnectionPool, PoolConfiguration}
import com.github.mauricio.async.db.postgresql.pool.PostgreSQLConnectionFactory
import com.github.mauricio.async.db.postgresql.util.URLParser
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.json4s.jackson.JsonMethods._
import org.json4s.{DefaultFormats, _}


object PostgresExport extends App {
  implicit val formats = DefaultFormats

  // Brings in default date formats etc.

  val sc = new SparkContext("local[8]", "Main")

  val configuration = URLParser.parse("jdbc:postgresql://localhost:5432/hackernews?user=postgres&password=bergersg")
  //val connection: Connection = new PostgreSQLConnection(configuration)

  private val factory = new PostgreSQLConnectionFactory(configuration)
  private val pool = new ConnectionPool(factory, PoolConfiguration.Default)

  //Await.result(connection.connect, 5 seconds)


  // Load all the files generated by the crawler and split each line (each line contains one article)
  // Get rid of weird characters and filter all the empty artciles (the ones that the crawler couldn't fetch)
  val corpus: RDD[Item] = sc.wholeTextFiles("articles/HN-stories-*").flatMap { case (_, file) =>
    file.split("\n").map(parse(_).extract[Item])
  }



  //    corpus.map { line =>
  //      val hnItem = line.HNItem
  //      val webPage = line.webpage
  //      connection.sendPreparedStatement(Queries.Insert,
  //        Array(
  //        hnItem.created_at, webPage.cleanedText, hnItem.author, hnItem.title, hnItem.points, hnItem.url,
  //          hnItem.objectID, webPage.title, webPage.metaDescription, webPage.metaKeywords,
  //            webPage.finalUrl, webPage.topImage
  //        )
  //      )
  //      println("Async query started!")
  //    }.collect()

  //  corpus.mapPartitions(partition => {
  //    println("Partition started")
  //    partition.map { line =>
  //      val hnItem = line.HNItem
  //      val webPage = line.webpage
  //
  //      Await.result(pool.sendPreparedStatement(Queries.Insert,
  //        Array(
  //          hnItem.created_at, webPage.cleanedText, hnItem.author, hnItem.title, hnItem.points, hnItem.url,
  //          hnItem.objectID, webPage.title, webPage.metaDescription, webPage.metaKeywords,
  //          webPage.finalUrl, webPage.topImage
  //        )
  //      ), 30 seconds)
  //    }
  //  }).collect()

  val test =
    corpus
    .mapPartitions(partition => {
    println("Partition started")
    partition.map { line =>
      val hnItem = line.HNItem
      val webPage = line.webpage

      Queries.Inserts +
        Array(
          hnItem.created_at, webPage.cleanedText, hnItem.author, hnItem.title, hnItem.points, hnItem.url,
          hnItem.objectID, webPage.title, webPage.metaDescription, webPage.metaKeywords,
          webPage.finalUrl, webPage.topImage
        ).mkString(", ") +
        ")"
    }
  }, true)
  .reduce((s1, s2) => s1 + "; " + s2)



  println(test)


  object Queries {
    val Insert =
      """
        |INSERT INTO articles(
        |            "Date", "Article", "Author", "Title", "Points", "Url",
        |            "ObjectId", "Webpage_title", "MetaDescription", "MetaKeywords",
        |            "FinalUrl", "TopImage")
        |    VALUES (?, ?, ?, ?, ?, ?,
        |            ?, ?, ?, ?,
        |            ?, ?)
      """.stripMargin

    val Inserts =
      """
        |INSERT INTO articles(
        |            "Date", "Article", "Author", "Title", "Points", "Url",
        |            "ObjectId", "Webpage_title", "MetaDescription", "MetaKeywords",
        |            "FinalUrl", "TopImage")
        |    VALUES (
      """.stripMargin
  }

}
