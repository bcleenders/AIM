package hackernewstrends

import edu.arizona.sista.processors.fastnlp.FastNLPProcessor
import org.apache.spark.SparkContext
import org.apache.spark.mllib.clustering.LDA
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD

import scala.collection.mutable

object Trends extends App {

  // Init the NLPProcessor (other options here: CoreNLPProcessor, BioNLPProcessor)
  val proc = new FastNLPProcessor(withDiscourse = true)

  // Split the articles into words, lemmatize everything
  // Filter out all words smaller than 3 characters, all non-characters
  // Lowercase the result
  def lemmatize(input: RDD[String], partitions: Boolean) = {
    if(!partitions) {
      input.map { p =>
        val doc = proc.mkDocument(p)
        proc.tagPartsOfSpeech(doc)
        proc.lemmatize(doc)
        val words = doc.sentences.map(x => x.lemmas.get).flatten
        val size = words.size
        doc.clear()

        println(s"Article lemmatized! (number of words = $size)")

        words
      }.map { words =>
        words
          .filter(_.length > 3) // Only words that are longer than 3 characters
          .filter(_.forall(java.lang.Character.isLetter)) // Only letters
          .filter(!stopWords.contains(_)) // Filter out the stop-words
          .map(_.toLowerCase) // convert everything to lowercase
      }
    }

    else {
      input.mapPartitions(partition => {
        partition.map { p =>
          val doc = proc.mkDocument(p)
          proc.tagPartsOfSpeech(doc)
          proc.lemmatize(doc)
          val words = doc.sentences.map(x => x.lemmas.get).flatten
          doc.clear()
          val size = words.size
          println(s"Article lemmatized! (number of words = $size)")

          words
        }
      }).map(words =>
        words
          .filter(_.length > 3) // Only words that are longer than 3 characters
          .filter(_.forall(java.lang.Character.isLetter)) // Only letters
          .filter(!stopWords.contains(_)) // Filter out the stop-words
          .map(_.toLowerCase) // convert everything to lowercase
        )
    }
  }

  def stem(input: RDD[String]) = {
    input.map( page => page.toLowerCase.split("\\s+")).map { word =>
      word
        .stem
        .filter(_.length > 3) // Only words that are longer than 3 characters
        .filter(_.forall(java.lang.Character.isLetter)) // Only letters
        .filter(!stopWords.contains(_)) // Filter out the stop-words
    }
  }

  val sc = new SparkContext("local", "Main")



  // Load all the files generated by the crawler and split each line (each line contains one article)
  // Get rid of weird characters and filter all the empty artciles (the ones that the crawler couldn't fetch)
  val corpus: RDD[String] = sc.wholeTextFiles("articles/HN-stories-2014-August-*").flatMap { case (_, file) =>
    file.split("}}").map(_.getArticle).filter(_ != "")
  }

  val tokenized = lemmatize(corpus, true)

  //corpus.foreach(println)
  println("Number of articles: ", corpus.count())

  // Load the stop words
  // List found on: http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop
  val stream = getClass.getResourceAsStream("/english.stop")
  val stopWords = scala.io.Source.fromInputStream(stream).getLines().toList


  // Choose the vocabulary.

  //   termCounts: Sorted list of (term, termCount) pairs
  val termCounts: Array[(String, Long)] =
    tokenized
      .flatMap(_.map(_ -> 1L))
      .reduceByKey(_ + _)
      .collect()
      .sortBy(-_._2)


  //   vocabArray: Chosen vocab (removing common terms)
  val numStopwords = 20
  val vocabArray: Array[String] =
    termCounts
      .takeRight(termCounts.size - numStopwords)
      .map(_._1)


  //   vocab: Map term -> term index
  val vocab: Map[String, Int] = vocabArray.zipWithIndex.toMap

  // Convert documents into term count vectors
  val documents: RDD[(Long, Vector)] =
    tokenized.zipWithIndex().map { case (tokens, id) =>
      val counts = new mutable.HashMap[Int, Double]()
      tokens.foreach { term =>
        if (vocab.contains(term)) {
          val idx = vocab(term)
          counts(idx) = counts.getOrElse(idx, 0.0) + 1.0
        }
      }
      (id, Vectors.sparse(vocab.size, counts.toSeq))
    }

  // Set LDA parameters
  val numTopics = 50
  val lda = new LDA().setK(numTopics).setMaxIterations(50)

  val ldaModel = lda.run(documents)
  val avgLogLikelihood = ldaModel.logLikelihood / documents.count()

  //ldaModel.topicDistributions.map(case ())


  // Print topics, showing top-weighted 10 terms for each topic.
  val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)
  topicIndices.foreach { case (terms, termWeights) =>
    println("TOPIC:")
    terms.zip(termWeights).foreach { case (term, weight) =>
      println(s"${vocabArray(term.toInt)}\t$weight")
    }
    println()
  }
}
