package hackernewstrends

import org.apache.spark.SparkContext
import org.apache.spark.mllib.clustering.LDA
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD
import org.json4s.jackson.JsonMethods._
import org.json4s.native.Serialization._
import org.json4s.{DefaultFormats, _}

import scala.collection.mutable

object Trends extends App {
  implicit val formats = DefaultFormats // Brings in default date formats etc.

  val jobQuery = "2015-*"
  val jobDescription = "LDA on all articles from 2015 with word stemming."
  // LDA parameters
  val numTopics = 50
  val maxIterations = 50

  // Load the stop words
  // List found on: http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop
  val stream = getClass.getResourceAsStream("/english.stop")
  val stopWords = scala.io.Source.fromInputStream(stream).getLines().toList

  val sc = new SparkContext("local[8]", "Main")


  // Load all the files generated by the crawler and split each line (each line contains one article)
  // Get rid of weird characters and filter all the empty artciles (the ones that the crawler couldn't fetch)
  val corpus: RDD[Item] = sc.wholeTextFiles("articles/HN-stories-" + jobQuery).flatMap { case (_, file) =>
    file.split("\n").map(parse(_).extract[Item]).filter(_.webpage.cleanedText != "")
  }

  //  println("Number of articles: ", corpus.count())
  //  println("Number of empty articles: ", corpus.filter(_.webpage.cleanedText == "").count())

  val tokenized =
    corpus
      .stem
      .map(words =>
      words
        .filter(_.length > 3) // Only words that are longer than 3 characters
        .filter(_.forall(java.lang.Character.isLetter)) // Only letters
        .filter(!stopWords.contains(_)) // Filter out the stop-words
        .map(_.toLowerCase) // convert everything to lowercase
      )


  // Choose the vocabulary.

  //   termCounts: Sorted list of (term, termCount) pairs
  val termCounts: Array[(String, Long)] =
    tokenized
      .flatMap(_.map(_ -> 1L))
      .reduceByKey(_ + _)
      .collect()
      .sortBy(-_._2)


  //   vocabArray: Chosen vocab (removing common terms)
  val numStopwords = 100
  val vocabArray: Array[String] =
    termCounts
      .takeRight(termCounts.size - numStopwords)
      .map(_._1)


  //   vocab: Map term -> term index
  val vocab: Map[String, Int] = vocabArray.zipWithIndex.toMap

  // Convert documents into term count vectors
  val documents: RDD[(Long, Vector)] =
    tokenized.zipWithIndex().map { case (tokens, id) =>
      val counts = new mutable.HashMap[Int, Double]()
      tokens.foreach { term =>
        if (vocab.contains(term)) {
          val idx = vocab(term)
          counts(idx) = counts.getOrElse(idx, 0.0) + 1.0
        }
      }
      (id, Vectors.sparse(vocab.size, counts.toSeq))
    }


  val lda = new LDA().setK(numTopics).setMaxIterations(maxIterations)

  val ldaModel = lda.run(documents)
  val avgLogLikelihood = ldaModel.logLikelihood / documents.count()


  // Write topics, showing top-weighted 20 terms for each topic.
  var counter = -1
  val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 20)
  val topics = topicIndices.map { case (terms, termWeights) =>
    counter = counter + 1
    val topicWords = terms.zip(termWeights).map { case (term, weight) =>
      TopicWord(vocabArray(term.toInt), weight)
    }

    Topic(counter, topicWords)
  }

  val outTM = new java.io.FileWriter(s"output/Stemming: Topic Matrix_$jobQuery.json")
  outTM.write(writePretty(Topics(jobQuery, jobDescription, topics)))
  outTM.close()


  val topicDist = ldaModel.topicDistributions.collect()
  val articles = corpus.map(_.HNItem.objectID).collect().zip(topicDist.sortBy(_._1)).map { case (docID, (_, topics)) =>
    val topTopics = topics.toArray.zipWithIndex.filter(_._1 >= 1.0 / numTopics).sortBy(-_._1).map { case (probability, topicID) =>
      TopTopic(topicID, probability)
    }

    ArticleTopic(docID, topTopics)
  }

  //Write top topics for each document
  val outTD = new java.io.FileWriter(s"output/Stemming: Topic Distributions_$jobQuery.json")
  outTD.write(writePretty(TopicDistribution(jobQuery, jobDescription, articles)))
  outTD.close()

  sc.stop()
}
