\section{Clustering Articles Into Topics}
\label{sec:topic_detection}

In order to cluster the articles into topics we chose two different unsupervised approaches: 

\begin{itemize}
\item \textbf{Latent Dirichlet Allocation}: clusters based on the content of the articles. This algorithm represents articles as a bag of words: order of words does not matter. Every article consists of words, and these words are linked to different topics with some probability.
\item \textbf{Word2Vec combined with k-means}: clusters the articles based on the titles. With a trained Word2Vec-model it is possible to transform every word into a vector. Transforming an article title into a vector is simply adding up the vectors of all words in the title. 
The vector-representations of the titles can be used as input for k-means, which will cluster the titles.
\end{itemize}

\subsection{Latent Dirichlet Allocation}
LDA is a generative probabilistic model of a set of documents. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words \cite{blei2003latent}. The input parameters are the number of topics ($k$) and the number of iterations ($N$) the algorithm will run. For each document in the collection, the words are assigned to topics in a two-stage process:

\begin{enumerate}
\item Temporary topics are assigned to each word in a semi-random manner (using the \textit{Dirichlet distribution}).
\item Repeat $N$ times: for every word $w$ in every document $d$ in the set of documents, do:
	\begin{enumerate}
		\item For each topic $t$: re-assign $w$ to $t$ with probability $P(t \mid d) \cdot P(w \mid t)$
	\end{enumerate}
\end{enumerate}

After running LDA, we can calculate the probability that a document $d$ is in a topic by looking at the probabilities words in $d$ occur in the topic. If an article can only be in one topic, simply pick the topic with the highest probabilty.

\subsection{Word2Vec}
Word2Vec is an algorithm that computes vector representations of words based on co-occurrences. The spatial distance between two word-vectors corresponds to word similarity. In order to achieve this it combines two different algorithms: skip-gram and continuous bag of words (CBOW).

Skip-gram model is an method for learning distributed vector representations that capture a large number of syntactic and semantic word relationships \cite{mikolov2013distributed}. Given a a word $w$, the skip-gram model predicts the $n$ neighboring words. The CBOW model works the other way around: it predicts $w$, given the $n$ neighboring words. 

Word2Vec is an example of ''shallow" learning and can be trained as a simple neural network. This neural network has a single hidden layer with no non-linearities and no unsupervised pre-training of layers is needed \cite{wang2014introduction}. 

\subsection{Measuring popularity of a topic}
First and foremost, our popularity must account for different lengths of months and a smaller userbase in the early years of Hacker News. To do so, we must use relative scores per month, rather than comparing absolute numbers.

We have decided to base our popularity measure on all three features we used for the rankings above: number of articles, number of upvotes and number of comments. If a topic receives 2\% of the arcticles, 2\% of the upvotes and 5\% of the comments, the popularity score is 3\% (the average of the three).

To give a formal definition: let $S_m$ be the set of all stories in month $m$ and $T_i$ all stories in the topic number $i$. With these two definitions, $S_m \cup T_i$ is the set of all stories on a topic $i$ in month $m$. Furthermore, let $s_u$ (resp. $s_c$) be the number of upvotes (resp. comments) story $s$ has received. Then, given a topic id $i$ and a month $m$, the score for that topic in that month is:

$$
	score(i, m) = 
		\frac{1}{3} \frac{|S_m \cup T_i|}{|S_m|} + 
		\frac{1}{3} \frac{\sum_{s \in S_m \cup \in T_i} s_u}{\sum_{s \in S_m} s_u}  + 
		\frac{1}{3} \frac{\sum_{s \in S_m \cup \in T_i} s_c}{\sum_{s \in S_m} s_c}
$$
