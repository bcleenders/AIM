\section{Dataset}
\label{sec:dataset}
As explained in the previous section, our analysis is on the set of all stories posted on Hacker News. It is a large set of news articles, blog posts, essays, tutorials and other types of mostly textual media. Nearly all of it is English, although there are a few other languages used as well.

Let us first describe what exactly we refer to when we use the word story. To do this, we will describe the four catagories of content on Hacker News:
\begin{itemize}
\item Stories: the majority of submissions are stories. A story can be either a link to another webpage or a relatively short text by the submittor.
\item Jobs: companies sponsored by YCombinator (the seed investor behind Hacker News) can post job offers on Hacker News. The percentage of jobs is very small: under one percent of the total volume.
\item Polls: users can submit multiple choice questions for other users to answer.
\item Comments: the three types mentioned above can receive comments by other users.
\end{itemize}

Since this research focuses only on the stories, we have left out the other three types. The dataset used in our research contains all stories between February 19th 2007 (the date Hacker News was launched\footnote{\url{https://news.ycombinator.com/hackernews.html}}) and June 10th 2015 (the day we ran our crawler). This is a time span of 3033 days, during which a total of over 1.5 million stories were submitted.

\subsection{Data Processing Steps}
The following steps were taken in the data processing:
\begin{enumerate}
	\item \textbf{Data retrieval}: crawl all data from the Hacker News API and the respective websites. We extracted the text from the HTML and wrote the result to disk as JSON files, split on days (one file contains one day of articles).
	\item \textbf{Exploratory research} (see section~\ref{sec:exploratory}): to get a better understanding of the dataset
		\begin{itemize}
			\item Postgres: to allow for more versatile access to the data, we stored the dataset in a Postgres database.
			\item Tableau: to visualize the results of our data analysis, we used Tableau.
		\end{itemize}
	\item \textbf{Train and apply clustering models} (see section~\ref{sec:topic_detection}): we used two ways of clustering the articles. Since our implementation of LDA did not scale properly, we only used the results of Word2Vec and k-means for the next steps.
	\item \textbf{Popularity calculation}: using the article and topic information, we can plot the popularity of a topic over time. We did this using Zeppelin\footnote{\url{http://zeppelin-project.org/}}, an Apache project for data analysis. We used Spark jobs and SparkSQL to analyze the data. This is a very scalable approach, but it requires some effort from the developer.
	\item \textbf{Interpretation}: (see section~\ref{sec:results}) since we had no automated way of linking the behaviour of the popularity (e.g. spikes or long-term in-/decrease) with real-world events, we interpreted this ourselves. We looked, for example, at the most popular articles in the months where popularity of a topic spiked.
\end{enumerate}

In the rest of this paper, we will further explain these steps and discuss the results of each step.

\subsection{Data Retrieval}
To crawl all stories, we used the official Hacker News API\footnote{\url{https://hn.algolia.com/api}}. This API returns some basic metadata about the story, such as the submitter, title, points (upvotes), a (possibly empty) story text and a (possibly empty) url. Stories generally either have a story text or a url, most only have a url.

For the stories with a non-empty story text field, fetching the story from the API is enough. For other stories, we also fetched the content linked to by the url. An important remark here is that some urls (especially the old ones) have become invalid over the course of time. Whenever the content was no longer available, we set the text to an empty string: the metadata (title, submitter, upvotes, etc.) is included in the statistics.

To crawl all stories (including the linked content), we used a homemade crawler that fetches the content, strips out meaningless text and html and saves the result in chunks of 1 day's worth of content. The code for this crawler is publicly available on GitHub\footnote{\url{https://github.com/bcleenders/AIM/tree/master/crawler}}.

The algorithm used for the extraction of meaningfull content is GoOse\footnote{\url{https://github.com/advancedlogic/GoOse}}. It uses heuristics to rank the importance and relevance of html elements on a webpage. For example: if it detects a \texttt{<div>...</div>} block with many words inside, then that is likely to be important. If, on the other hand, it finds an html block \texttt{<button>Login</button>}, then it will remove the block for it is probably not a relevant part of the text of the page.

The resulting dataset is about 4.4 GB in size.
