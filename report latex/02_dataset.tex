\section{Dataset}
\label{sec:dataset}
As explained in the previous section, our analysis is on the set of all stories posted on Hacker News. It is a large set of news articles, blog posts, essays, tutorials and other types of mostly textual media. Nearly all of it is English, although there are a few other languages used as well.

Let us first describe what exactly we refer to when we use the word story. To do this, we will describe the four catagories of content on Hacker News:
\begin{itemize}
\item Stories: the majority of submissions are stories. A story can be either a link to another webpage or a relatively short text by the submittor.
\item Jobs: companies sponsored by YCombinator (the seed investor behind Hacker News) can post job offers on Hacker News. The percentage of jobs is very small: under one percent of the total volume.
\item Polls: users can submit multiple choice questions for other users to answer.
\item Comments: the three types mentioned above can receive comments by other users.
\end{itemize}

Since this research focuses only on the stories, we have left out the other three types. The dataset used in our research contains all stories between February 19th 2007 (the date Hacker News was launched\footnote{\url{https://news.ycombinator.com/hackernews.html}}) and June 10th 2015 (the day we ran our crawler). This is a time span of 3033 days, during which a total of over 1.5 million stories were submitted.

\subsection{Data Processing Steps}
The following steps were taken in the data processing:
\begin{enumerate}
	\item \textbf{Data retrieval}: crawl all data from the Hacker News API and the respective websites. After fetching the data, we extracted the text from the HTML using GoOse. The result was written to disk as JSON files, split on days (one file contained one day of articles).
	\item \textbf{Exploratory research} (see section~\ref{sec:exploratory})
		\begin{itemize}
			\item Postgres: to allow for more versatile access to the data, we stored the dataset in a Postgres database.
			\item Tableau: to visualize the results of our data analysis, we used Tableau.
		\end{itemize}
	\item \textbf{Train and apply models}: 

	\item \textbf{Trend detection}: using the article information and article/topic tuples, we can calculate the popularity of a topic in a given month. By plotting charts for topics, we could compare We did this with two different techniques:
		\begin{itemize}
			\item \textbf{Zeppelin}: an Apache project for data analysis. We used Spark jobs and SparkSQL to analyze the data. This is a very scalable approach, but it requires some effort from the developer.
			\item \textbf{Postgres}: the database of all stories and topics is several gigabytes in size. Although that's pretty large, a single database can still handle it. This allowed for some easier querying of the data since Postgres supports more SQL functions than SparkSQL.
		\end{itemize}
\end{enumerate}

\subsection{Data Retrieval}
To crawl all stories, we used the official Hacker News API\footnote{\url{https://hn.algolia.com/api}}. This API returns some basic metadata about the story, such as the submitter, title, points (upvotes), a (possibly empty) story text and a (possibly empty) url. Stories generally either have a story text or a url, most only have a url.

For the stories with a non-empty story text field, fetching the story from the API is enough. For other stories, we also fetched the content linked to by the url. An important remark here is that some urls (especially the old ones) have become invalid over the course of time. Whenever the content was no longer available, we set the text to an empty string: the metadata (title, submitter, upvotes, etc.) is included in the statistics.

To crawl all stories (including the linked content), we used a homemade crawler that fetches the content, strips out meaningless text and html and saves the result in chunks of 1 day's worth of content. The code for this crawler is publicly available on GitHub\footnote{\url{https://github.com/bcleenders/AIM/tree/master/crawler}}.

The algorithm used for the extraction of meaningfull content is GoOse\footnote{\url{https://github.com/advancedlogic/GoOse}}. It uses heuristics to rank the importance and relevance of html elements on a webpage. For example: if it detects a \texttt{<div>...</div>} block with many words inside, then that is likely to be important. If, on the other hand, it finds an html block \texttt{<button>Login</button>}, then it will remove the block for it is probably not a relevant part of the text of the page.

The resulting dataset is about 4.4 GB in size.
